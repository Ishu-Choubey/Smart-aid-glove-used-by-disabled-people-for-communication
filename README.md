# Smart-aid-glove-used-by-disabled-people-for-communication
ABSTRACT
It has been observed that disabled people like deaf and mute have hard time communicating and coping up with normal people due to their inability to speak. So, the only way in which they express their emotions or convey their thoughts is by using sign language, but in today’s world very few people understand sign language. Approximately 10% of the world’s population is mute or deaf, hence they use sign language as their primary language. Looking at their struggle to communicate led us to the idea of building this project. In this era of advancement, it is very essential to use our technology wisely and in a feasible way, such that it can help people of various stands in their daily life. This project involves a smart glove embedded with multiple flex sensors for precise figure bending or movement of fingers which will send data to microcontroller ARDUINO. The data thus obtained gets processed in microcontroller. Based on that, the hand sign language will be decoded using a predefined database. After this the output gets displayed in an on-board LCD display for the user to view and also gets transmitted in audio and text format to smartphone of the person user’s talking to.
INTRODUCTION
Advancement in Technology has given many answers to many unsolvable problems. Technology advancement not only gives answers to unsolvable questions but also gives new ideas to create new things. Development of the Glove based gesture control recognition system started many years ago for different purposes. Communication is the interchange of data among two or more entities informed by voice, signals, data etc. We can use glove-based gesture control systems for communication purposes to give voice to voiceless people. For mute people Hand sign language is the only way to communicate to the outside world. 
According to recent surveys in India, millions of individuals have hearing and speaking disabilities, with estimates suggesting that this number could reach 2.4 million. For these individuals, communicating with others and expressing emotions can be challenging. While sign language is commonly used among disabled people for communication, it is difficult for those without disabilities to understand and learn. This communication barrier can lead to isolation and difficulty expressing one's opinions. Additionally, paralyzed individuals may struggle to communicate and move freely. To address this challenge, we have designed an assistance glove for disabled that enables individuals to communicate without assistance. Our approach uses the data obtained from glove-based gesture recognition system that detects finger movement through flex sensors. This approach provides accurate and feasible results. Our fingers have three movable joints, using different combinations of these joints we can make a wide range of gestures. Five flex sensors have been used to obtain this and in order to increase the number of combinations and gesture by four times we have integrated this model with an accelerometer i.e., MPU 6050. We read the data from these actuators using Arduino UNO board placed on the glove as shown in fig.2 which provides faster response along with great storage capacity. It then collects, analyses, and displays the data on an LCD screen attached to it. It can also be used to connect with an Android app using a Bluetooth module i.e., HC-05. In the android app the output sign gesture is displayed in text and speech format. By utilizing wireless communication between the Arduino Uno (user end) and Smartphones (taking to), our glove provides a practical solution for individuals with disabilities.
LITERATURE SURVEY
In [1] Laura-Bianca Bilius et al conducted a review on algorithms which are based on gesture recognition. Based on this review they proposed a new gesture recognition algorithm known as TIGER (Tucker-based Instrument for Gesture Recognition). This model is based on their old model known as Tucker2 as well. In this the data is collected using an inertial sensor after that they implement Tucker decomposition for efficient motion gesture detection and achieve dimensionality reduction. In [2] Chengshuo Xia et al developed a novel idea in which they use distance sensors to detect hand gestures. The algorithm uses real time data to detect the real movement as a result this decreased the requirement of data samples. Later they also optimised this method to enhance sensor position and reduce the number of input samples. In [3] Pukar Maharjan et al inspired by the highly sensitive tactile performance of human skin developed a self-powered flex sensor based sign language detector which charges thermally using body heat. This is a micro level technology which is sensitive, better stable, and easy-to-manufacture and self-powered. In [4] K. Sangeethalakshmi et al developed a vocalizer based on a microcontroller that aims to convert gestures into noticeable sounds, reducing the difficulties of dissimilar hand sign conventions used all around the globe.  In [7] M. Neela Harish and S. Poonguzhali represented brief research on gesture detection gloves and its application in heat insulation and thermal resistance, the paper contains detailed review for various gloves used in different areas such as surveying, robotics, gaming, motion and imaging, and discuss its various applications. In [10] Ivo Stančić et al developed a mobile robot which can be controlled from anywhere in outdoor or indoor environment using hand gesture movements, the movement of the glove or gestures determine the movement of the robot model, this system is useful for remote or precise works or applications where human interaction is not possible or dangerous.
In [5] Fazil Salman et al designed a robotic hand motion system which can be controlled wirelessly. The system consists of a glove with force sensors and a robotic arm which is 3D printed. After wearing the glove the wielder could guide the motion of the 3D printed robotic arm. This system is useful for remote or precise works or applications, the 3D printed robotic arm follows glove motion with a maximum latency of 0.133ms. In [6] Xianjie Pu et al have demonstrated a new type of robotic-human hand based passive motion detection and control algorithm. They developed a triboelectric sensor which can be worn on a finger to actively detect the finger joint's flexion-extension degree, direction and speed in real time. The triboelectric sensor while operating in grating-sliding mode counts pulse and by comparing the positive and negative pulses respectively to represent flexion and extension respectively the angular position of the finger joints can be obtained with an absolute value.mIn [8] Elisa Morganti et al developed a smart watch design for sensing of gestures with objects, using RFID technology this embedded system recognises tagged objects. An easy to implement solution is discussed and presented. In [15] M.K. Prabhu et al proposed a human look alike gesture controlled robotic arm where the movement done by the human palm is copied by the robotic arm. The various movements which were inferred from human hand movement were used for the implementation of this project. Five flex sensors have been used to imitate the gestures by human fingers. In [16] P. Sivaraman et al with the help of the Embedded system proposed a human looking gesture controlled robotic arm where the movement done by the human palm is copied by the robotic arm with the help of an embedded system. The various movements which were inferred from human hand movement were used for the implementation of this project. Flex sensors are attached to the gloves which control the position of the robotic arm. In [17] Elisa Digo et al worked on the improvement of collaborative robotics by executing an appropriate behaviour model for the robot and human within a workspace. With the help of IMU they studied and worked on the real time estimation method for joint kinematics.
In [9] Mehmet Akif Ozdemir et al proposed an idea to detect hand gesture movements using EMG signals(electromyography), in this paper they developed a data set and according to that using machine learning algorithms they achieve results. In [11] Shuo Jiang et al put up a novel idea using EMG and FMG signal sensing approach to determine hand gesture recognition and classification. They developed an armband prototype which is used as an input device. FMG and EMG signals are biological signals generated by our body when muscles move. In [13] Feiyun Xiao et al made a real time motion detection and intention recognition system, unlike others they implemented using a minimum number of surface electromyography sensors placed on hand and wrist. Based on these current signals from EMG sensors this prediction in gesture movement is made. In [14] Qiyu Li and Reza Langari developed an EMG signal based human computer interaction interface using convolutional neural networks with “long short-term memory” for a dynamic approach to hand gesture recognition. In [20] Zhongming Lv et al worked towards the extraction of information from the surface electromyographic signals and understand the human gesture by reading its movements, they proposed an algorithm based on self-organising mapping network (SOM) and radial basis neural network (RBF) for quality and feature extraction and also reduce size of the complex structure. Finally they defined a pattern classification using previous EMG analysis and created a dataset for motion detection comparison.
In [12] Samer Alashhab et al proposed a real time interactive system which can be controlled by hand gestures to help people with vision disability. They achieved this using an efficient multi-task neural network architecture. They maintained a dataset containing more than 40k images, using these images they properly trained their algorithm. In [18] Ali.H. Alrubayi et al used the Machine learning techniques for recognizing the static gestures in Malaysian Sign language. The whole work was divided into two phases namely Data acquisition and Data Processing. The first phase consisted of accepting the data, i.e. understanding the shape and orientation of data. After this the features are measured and understood using ML techniques. Around 10 of them have been used for Sign language gesture recognition. In [19] Thiago Simões Dias et al compared the Recurrent Neural Network (RNN) and handcraft feature extraction. They also experimented with ensembles and GRUs which gave the best output. For conventional methods RNN was the best method. They used the usual method of pattern recognition with the glove to conduct their experiment. 
PROPOSED METHODOLOGY
A smart aid glove is a wearable technology device that is designed to assist people with disabilities in communicating effectively. The glove uses flex sensors and a Bluetooth module to detect hand movements and transmit data wirelessly to a computer or a mobile device. In this essay, we will discuss the methodology behind a smart aid glove for disabled people using flex sensors and a Bluetooth module.
The smart aid glove comprises several flex sensors, a Bluetooth module, a microcontroller Arduino UNO R3, and accelerometer. Flex sensors are resistive sensors that change their resistance when they are bent or flexed. They are commonly used to detect the bending of fingers and other hand movements. The Bluetooth module is used to transmit data wirelessly to a computer or a mobile device. The microcontroller Arduino UNO R3 processes the data and displays the date on the LCD screen and user’s device. 
The first step in building a smart aid glove is to design the glove and attach the flex sensors to the glove's fingers. The flex sensors are calibrated to detect the user's hand movements and share the message they want to share effectively and accurately. The Bluetooth.
The System Works As Follows
Step 1: Sensor data acquisition: The flex sensors and accelerometer are attached to a glove, which is worn on the hand. As the fingers and the hand move, the sensors detect changes in the angle, orientation, and movement, and generate analog signals. These signals are then fed into the microcontroller.
Step 2: Signal processing: The microcontroller receives the sensor data and performs the following operations:
•	Analog signal mapping: The microcontroller scales down the analogue signals from the sensors into values that can be processed by the system.
•	Filtering and noise reduction: 10K ohm resistors are used as pull up resistors with flex sensors to filter out any noise or interference from the sensor data, to ensure accurate measurements.
•	Gesture recognition: The microcontroller compares the sensor data with pre-recorded values stored in its memory, corresponding to different hand gestures. If a match is found, the microcontroller identifies the gesture and proceeds to the next step.
Step 3: Display of recognized gesture: The microcontroller controls the LCD display, which displays the recognized hand gesture or other information, such as battery level or Bluetooth connection status.
Step 4: Bluetooth transmission: Once the microcontroller has recognized a hand gesture, it sends a signal to the Bluetooth module, which wirelessly transmits the data to a remote device. The Bluetooth module uses a specific protocol, such as Bluetooth Low Energy (BLE) or Bluetooth Classic, to establish a connection with the receiver device.
Step 5: Gesture interpretation: The receiver device, such as a smartphone or a computer, receives the data transmitted by the Bluetooth module. The receiver device then interprets the data in the form of text and audio using the software.
Step 6: Power supply management: The system can be designed to be powered by a battery or an external power source. The microcontroller and the Bluetooth module can be programmed to conserve power by entering low-power modes when idle.
In summary, the proposed model and working of a flex sensor and accelerometer-based hand recognition system with an LCD display, which sends data through a Bluetooth module, involves the use of flex sensors and an accelerometer to detect changes in the angle, orientation, and movement of the hand, a microcontroller to process the sensor data and control the LCD display and Bluetooth module, an LCD display to display the recognized hand gestures or other information, a Bluetooth module to wirelessly transmit the data to a remote device, and software to interpret the data and recognize the hand gestures. The system can be powered by a battery or an external power source and can be designed to conserve power by entering low-power modes when not in use.
 
FIGURE 1. Block Diagram.
Expected Outcomes
•	The person wearing the glove will be able to communicate using it even with those who don’t understand sign language.
•	He can review the transmitted gesture using an onboard LCD screen.
•	The person in front of the bearer will be able to see and hear the real-time gestures using their own device.
HARDWARE CIRCUIT AND SIMULATION
 
FIGURE 2. Top view of the hardware setup.

 
FIGURE 3. Accelerometer positioning on the palm side.

FIGURE 4. Side view of the model. 


 
FIGURE 5.  Flex sensor positioning on the back side
FIGURE 6. Hardware Circuit setup
 
Video Link: https://drive.google.com/drive/folders/1Zm-oJDdD8gsDMUan2JvGk35dS-ZvptOa
DISCUSSION AND COMPARISON
There are a number of research papers available on hand recognition systems that use flex sensors, accelerometers, and Bluetooth modules. In [21] five flex sensors have been used to capture the finger movements, and the data was processed using a support vector machine classifier. The system achieved an accuracy of 96.5% for six different hand gestures. Whereas in this paper, using this method we can represent over 500 gestures using a combination of 5 flex sensors paired with a 4 directional accelerometer. Here we demonstrated that hand recognition systems based on flex sensors and accelerometers can achieve high accuracy rates, and that including an LCD display and Bluetooth module can enable real-time feedback and remote-control capabilities.  
In [22] a hand gesture recognition system has been presented that uses both flex sensors and an accelerometer to capture the movement of the user's hand. The system includes an LCD display to show the recognized gesture and a Bluetooth module to transmit the data to a remote device. The study evaluated the system using a dataset of 10 gestures and achieved an accuracy of 87%. Whereas in this paper the proposed model can recognize more than 500 gestures using 5 flex sensors combined with an accelerometer.
In [7] the authors discuss various types of gloves, their sensing mechanisms, and the accuracy of gesture recognition achieved. They also argue that these gloves can be used to monitor temperature and improve safety in various industries. However, the paper lacks experimental data or results to support their claims. Whereas in this paper when results are verified experimentally. However, the specific design and performance of these systems can vary depending on factors such as the quality of the sensors and the algorithms used for gesture recognition.
CONCLUSION
Many people with disabilities like deaf and mute or those with cerebral palsy or paralysis, may have difficulty communicating verbally. We successfully implemented a smart aid glove that can help in bridging this communication gap by allowing the user to communicate through hand gestures and movements.
The smart aid glove is a revolutionary wearable technology device that can significantly improve the lives of people with disabilities. With its flex sensors, Bluetooth module, microcontroller Arduino UNO R3 board, and accelerometer, the smart aid glove helps in effective communication of disabled pic like mute and deaf by sharing whatever message they want to share on an LCD board and sending their message via Bluetooth module to user’s device. It can provide feedback in real-time, enabling people with disabilities to communicate effectively even with those people who don’t understand sign language. This increases their sense of independence and self-confidence, thereby improving their overall quality of life. The smart aid glove is a perfect example of how technology can be used for the betterment of society and has the potential to transform the lives of millions of people with disabilities.
